#!/bin/bash

echo "=== [1] Iniciando Ollama para uso com Open WebUI ==="

# Parar qualquer inst√¢ncia anterior do Ollama via systemd
echo "‚û° Parando servi√ßo systemd do Ollama (caso esteja rodando)..."
sudo systemctl stop ollama 2>/dev/null

# Iniciar o Ollama expondo externamente na porta 11434
echo "‚û° Iniciando ollama com OLLAMA_HOST=0.0.0.0:11434"
OLLAMA_HOST=0.0.0.0:11434 ollama serve &

# Aguarda alguns segundos para o servidor iniciar
sleep 3

# Verifica os modelos dispon√≠veis
echo "‚û° Verificando modelos j√° dispon√≠veis:"
curl -s http://0.0.0.0:11434/api/tags | jq .

# Baixando modelos
echo "‚û° Baixando modelos DeepSeek..."
ollama pull deepseek-r1:7b
ollama pull llama
ollama pull gemma:2b

# Puxar imagem atualizada do Open WebUI
echo "‚û° Atualizando imagem Docker do Open WebUI..."
sudo docker pull ghcr.io/open-webui/open-webui:main

# Verifica se cont√™iner j√° existe
EXISTE=$(sudo docker ps -a -q -f name=open-webui)

if [ "$EXISTE" ]; then
  echo "‚û° Cont√™iner 'open-webui' j√° existe. Reiniciando..."
  sudo docker rm -f open-webui
fi

# Executar o cont√™iner Open WebUI
echo "‚û° Iniciando cont√™iner do Open WebUI em http://localhost:3000 ..."
sudo docker run -d -p 3000:8080 \
  --add-host=host.docker.internal:host-gateway \
  -e OLLAMA_HOST=http://host.docker.internal:11434 \
  -e OLLAMA_MODELS=/root/.ollama/models \
  -v open-webui:/app/backend/data \
  --name open-webui --restart always \
  ghcr.io/open-webui/open-webui:main

echo ""
echo "‚úÖ Open WebUI est√° rodando em: http://localhost:3000"
echo "‚úÖ Ollama (API REST) dispon√≠vel em: http://0.0.0.0:11434"
echo ""

# Mostrar status dos servi√ßos
echo "‚û° Status atual:"
curl -s http://0.0.0.0:11434/api/status | jq .

echo ""
echo "üîç Para logs do Ollama (via systemd): journalctl -u ollama -f"
echo "üîç Para logs do Open WebUI: sudo docker logs -f open-webui"
